Quantified Self: Predicting Exercise Execution Class with Machine Learning Techniques
===================================================================================
## Summary
The quantified self movement is a rapidly growing field where individuals attempt to
measure activity levels of a variety of daily activities.  Fueled by the explosion 
of cheap sensors, these efforts to rigourously track activity have had great success in
quantifying how much or which activity you are performing.  The next frontier is objectively
addressing how well you are performing the activity. 

This report uses data from http://groupware.les.inf.puc-rio.br/har to attempt to classify
the manner in which an individual executed a Unilateral Dumbbell Biceps Curl. The exercise 
repetitions are classified into one of the following groups:

* A - exactly according to the specification
* B - throwing the elbows to the front
* C - lifting the dumbbell only halfway
* D - lowering the dumbbell only halfway
* E - throwing the hips to the front

After fitting several machine learning models, a 5-fold cross validated random forest 
model was identified as the top performing model.  Using the random forest model, the out of 
sample error rate was reduced to 0.17%.

## Loading Data and Necessary Packages

```{r, package_load}
suppressMessages(library(caret))
suppressMessages(library(ggplot2))
suppressMessages(library(GGally))
```
Data is loaded in and dimensions are checked. 
```{r, data_import, cache = TRUE}
train_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
if(!file.exists('pml-training.csv')){
  download.file(train_url, destfile = 'pml-training.csv')
}
test_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
if(!file.exists('pml-testing.csv')){
  download.file(test_url, destfile = 'pml-testing.csv')
}
training <- read.csv('pml-training.csv', na.strings = c("NA", "", "#DIV/0!"))
testing <- read.csv('pml-testing.csv', na.strings = c("NA", "", "#DIV/0!"))
dim(training)
```

## Data Exploration and Cleaning
```{r}
str(training, list.len = 20)
table(training$classe)
```
For this analysis we are predicting the class of activity the user is performing--
A, B, C, D, or E.  The class split is even enough that I don't think we need to address 
class imbalance before we start modeling.

Looking at the data set, it appears the first several variables aren't needed.  'X'
is just an index of the observations and the timestamp variables are different representations
of the time the activity was performed.  While in a production setting these could be of
predicitive value, here, we want to identify activity type from sensor data. 
I remove all these variables for my analysis.
```{r}
training <- training[, -c(1, 3:6)]
```

```{r}
prop.table(table(training$user_name, training$classe), 1)
```

'User_name' may or may not be predictive. If we look at the proportion of a users 
activity by class, the proportions stay relatively constant between individual users.
Since all of our data is about these six individuals this variable appears useful.  
If we were trying to generalize a model to predict the class of activity by a new set
of users I would remove this variable.  For now, I will keep it.

#### Removing Variables with Excesssive NA Values
There are numerous NA values within the data.  For this relatively simple analysis, I 
am not going to dive too deeply into whether the presence of NA values is informative.
If a variable is greater than 90% NA values, I remove the variable.

```{r, na_removal}
perc_na_greater_90 <- apply(!is.na(training), 2, sum) > nrow(training)* .9
training <- training[, perc_na_greater_90]
dim(training)
```

#### Removing Highly Correlated Variables
We are now down to a more managable number of variables for this analysis.
Let's remove highly correlated variables so we can test out how they affect linear 
models.

```{r, cor_removed}
correlated <- cor(training[, -c(1, 55)])
high_cor <- sort(findCorrelation(correlated, cutoff = .9))
low_cor_var <- training[, -c(1, high_cor)]
```
#### Exploratory Plot
To get a sense for the data, I plot a few of our features and examine the clustering.

```{r, exploratory_plots, cache = TRUE, fig.width = 12, fig.height = 12}
ggpairs(training, columns = 10:14, color = 'classe', upper = 'blank', alpha = .1,
        title = "Exploratory Variable Plots")
```

As you can see, there is significant overlapping, but clusters are evident within this
subset of variables.

## Creating training and validation set
For this report, I split my training data into a training set and a cross-validation set.
I do this for my data set with high correlations removed and for the full data set. After
training all my models with k-fold cross validation on this training set, I will perform
my out of sample error estimate using the cv data set.  This will give me a more accurate 
estimation of the performance of my models.
```{r}
set.seed(123)
index <- createDataPartition(training$classe, p = .7, list = FALSE)
train_full <- training[index,]
cv_full <- training[-index,]
train_uncor <- low_cor_var[index,]
cv_uncor <- low_cor_var[-index,]
```

By default, trainControl in the caret package uses 25-fold cross validation.  For our 
simple models here that seems unnecessary. I went with 5-fold cross validation to 
reduce computation time.

```{r, cross_validation}
set.seed(123)
ctrl <- trainControl(method = 'cv', number = 5)
```

## Model Fitting
I decided to test out three models on the data: linear discriminant analysis, stochastic
gradient boosting (gbm) and random forests. All models were fit with the base parameters
and no tuning was done to improve performance.  Models were fit using the training split
created above with 5-fold cross validation.  Out of sample error estimates were generated
using the cv split created above.

#### Linear Discriminant Analysis Model
```{r, lda_fit, cache = TRUE, results = 'hide'}
lda_fit <- train(classe ~ ., method = 'lda', trControl = ctrl,
                 data = train_uncor)
lda_fit_full <- train(classe~ ., method = 'lda', trControl = ctrl,
                      data = train_full)
```

After fitting both LDA models, I use the cross-validation set to estimate the out of 
sample error rate.
```{r, cache = TRUE}
lda_pred <- predict(lda_fit, newdata = cv_uncor)
lda_conf_mat <- confusionMatrix(lda_pred, cv_uncor$classe)

lda_pred_full <- predict(lda_fit_full, newdata = cv_full)
lda_conf_mat_full <- confusionMatrix(lda_pred_full, cv_full$classe)
```

The out of sample accuracy for these models is `r round(lda_conf_mat$overall[1], 4)*100`%
for the LDA model with correlated variables removed and `r round(lda_conf_mat_full$overall[1], 4)*100`%
for the LDA model with all variables included.  74% accuracy for this relatively simple
model is impressive, but we can definitely do better.

#### Gradient Boosting Machines
```{r, gbm_fit, cache = TRUE, results = 'hide', message = FALSE}
gbm_fit_full <- train(classe ~ ., method = 'gbm', trControl = ctrl, data = train_full,
                      verbose = TRUE)
```

```{r}
gbm_pred_full <- predict(gbm_fit_full, newdata = cv_full)
gbm_conf_mat <- confusionMatrix(gbm_pred_full, cv_full$classe)
```

Using the same cross-validation setup used in the LDA models, the out of sample accuracy
is `r round(gbm_conf_mat$overall[1], 4)*100`%. This is more than sufficient for this 
analysis, but let's see what random forests can do.

#### Random Forests
```{r, rf_fit, cache = TRUE, results = 'hide', message = FALSE}
set.seed(123)
rf_fit_full <- train(classe ~., method = 'rf', trControl = ctrl, data = train_full, 
                     do.trace = TRUE)
```

```{r, message = FALSE}
rf_pred_full <- predict(rf_fit_full, newdata = cv_full)
rf_conf_mat <- confusionMatrix(rf_pred_full, cv_full$classe)
```

Our random forest model is the winner by a slight margin with an out of sample accuracy of
`r round(rf_conf_mat$overall[1], 4)*100`%.  Since this is our best performer, let's have a 
look at the full confusion matrix.
```{r, echo = FALSE}
rf_conf_mat
```

We have good performance across all classes with high levels of specificity and sensitivity.
Overall it looks like a very good model fit and we would expect to have about `r round(1- rf_conf_mat$overall[1], 4)*100`%
out of sample error.

## Conclusions
Using a random forest model, I was able to predict user activity class with `r round(rf_conf_mat$overall[1], 4)*100`%
accuracy.  While this is great, I am skeptical of the results.  It is highly unusual 
to have a model with such high performance.  Perhaps there is some underlying structure
to the dataset that the model is identifying to base its predictions on.  Regardless,
for the purposes of this analysis, the model was a success.  A next step would be applying
the model to data from a new group of users and seeing how the model performs.